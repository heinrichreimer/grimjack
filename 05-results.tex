\section{Results}
\label{results}

We evaluate our approach by effectiveness to retrieve relevant passages, to retrieve high-quality passages, and to predict the correct stance towards the compared objects, using manual judgments by the Touché organizers. \citet{BondarenkoFKSGBPBSWPH2022} asked human volunteers to label each document pooled from all submitted runs up to depth~5 with respect to relevance~(0=not relevant, 1=relevant, 2=highly relevant), rhetorical quality~(0=low quality or not argumentative, 1=sufficient quality, 2=high quality), and stance to the compared objects~(pro first object, pro second object, neutral, no stance).

\input{table-results-relevance}
\input{table-results-quality}
The results for relevance and quality effectiveness~(Tables~\ref{table-results-relevance} and~\ref{table-results-quality}) indicate that our baseline run~(grimjack-baseline) using query likelihood with Dirichlet smoothing performs worse than the BM25~baseline~(Puss in Boots~\cite{BondarenkoFKSGBPBSWPH2022}). Because most of our runs re-rank retrieved results from the initial query likelihood retrieval, we focus our evaluation on the differences in nDCG@5 induced by the individual re-ranking stages. Nonetheless, we acknowledge that all of our submitted runs are outperformed by the BM25~baseline and other dense rankers participating in the shared task~(Captain Levi and Aldo Nadi).
The differences in nDCG@5 performance compared to our query likelihood baseline indicate that axiomatic re-ranking can increase consistency with argumentativeness axioms while retaining equal retrieval effectiveness. Unfortunately, query expansion with T0++ slightly decreases nDCG@5 performance on average by about 3\,p.p. for relevance judgments and 2\,p.p. for quality judgments. Stance-based re-ranking, however, can increase nDCG@5 effectiveness by up to 5\,p.p. for relevance judgments and by 4\,p.p. for quality judgments. None of our re-ranking stages can sufficiently compensate for the worse retrieval performance of the initial query likelihood ranking.

\input{table-results-stance}
For stance classification, we compare the T0-based stance classification approach with the best competing team's approach~(Captain Levi, Ro\Bert{}a without fine-tuning) and the baseline~(Puss in Boots) that predicts the majority class~(no stance). In Table~\ref{table-results-stance}, we report the macro-averaged $F_1$~score per run and per team as well as the number of documents~$N$ for which the predicted stance has a ground-truth label. We observe that because only the top-5 passages were pooled for manual judgments only a limited number of predicted stance labels~(e.g. 1208~for run~4) can be used for evaluation, even though we predicted the stance up to depth~100~(i.e. 5000~predicted stance labels per run). In this setting our run~4~(i.e. stance prediction using T0++; cf.\ Section~\ref{argument-tagging}) has the best macro-averaged $F_1$ score of all submitted runs. However, due to the limited number of labels available for evaluation and because the number of available labels for evaluation differs across teams and runs, we cannot directly compare different runs by $F_1$ score when considering all predicted stance labels. For example, the 3792~unjudged labels from run~4 could be correctly predicted~(i.e., increasing $F_1$-performance) or incorrectly predicted~(i.e., decreasing $F_1$-performance). As an alternative, comparable measure, in the leftmost columns of Table~\ref{table-results-stance}, we look at the $F_1$ score of predicted stances of only the top-5 passages of each run. All 250~stance labels from the top-5 results of each submitted run have corresponding ground-truth labels due to the top-5 pooling for manual judgment.
When considering only the top-5 passages, our stance classification approach using T0++ falls behind Team Captain Levi's and Team Katana's best performing approaches.
However, we acknowledge that using only 250 samples to compare classifier performance might be an insufficient sample size. Due to considering only top results, the evaluation could also be biased towards classifiers that are tuned to work better on (highly) relevant documents but worse on less relevant documents. We argue that it is therefore not possible to conduct an unbiased comparison of the different stance classification approaches submitted to the Touché task~2 unless all teams would submit their stance predictions for an independent stance prediction dataset .

\redtext{NEEDS ACTUAL RESULTS}

\subsection{Transferring Relevance Judgements from Touché~2020--2021}
\label{transfer-relevance-judgements}

We also experiment with transferring the relevance judgements from previous editions of the Touché shared task~\cite{BondarenkoFBGAPBSWPH2020,BondarenkoGFBAPBSWPH2021} in order to increase the amount of judgments to test against.
In~2020 and~2021, however, the results were judged on the document level, not on the passage level as in this year's edition.
To make use of the document-level judgments from Touché~2020--2021, we retrieve passages from this year's extracted passage dataset~\cite{BondarenkoFKSGBPBSWPH2022} but for the topics from previous editions.
We then assume that if a document was relevant all passages extracted from that document are relevant as well, that is, the passages inherit their containing document's relevance label.
Unfortunately, we could only match 16.76\,\% of the top-100 passages retrieved by our baseline with document-level relevance labels from Touché~2020--2021.
Because of the insufficient label coverage we do not include a more detailed evaluation of effectiveness using previous editions' judgements.
