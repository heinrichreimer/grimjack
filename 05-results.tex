\section{Results}
\label{results}

We evaluate our approach by effectiveness to retrieve relevant passages, to retrieve high-quality passages, and to predict the correct stance towards the compared objects, using manual judgments by the Touché organizers. \citet{BondarenkoFKSGBPBSWPH2022} asked human volunteers to label each document pooled from all submitted runs up to depth~5 with respect to relevance~(0=not relevant, 1=relevant, 2=highly relevant), rhetorical quality~(0=low quality or not argumentative, 1=sufficient quality, 2=high quality), and stance to the compared objects~(pro first object, pro second object, neutral, no stance).

\input{table-results-relevance}
\input{table-results-quality}
The results for relevance and quality effectiveness~(Tables~\ref{table-results-relevance} and~\ref{table-results-quality}) indicate that our baseline run~(grimjack-baseline) using query likelihood with Dirichlet smoothing performs worse than the BM25~baseline~(Puss in Boots~\cite{BondarenkoFKSGBPBSWPH2022}). Because most of our runs re-rank retrieved results from the initial query likelihood retrieval, we focus our evaluation on the differences in nDCG@5 induced by the individual re-ranking stages. Nonetheless, we acknowledge that all of our submitted runs are outperformed by the BM25~baseline and other dense rankers participating in the shared task~(Captain Levi and Aldo Nadi).
The differences in nDCG@5 performance compared to our query likelihood baseline indicate that axiomatic re-ranking can increase consistency with argumentative axioms while retaining equal retrieval effectiveness. Unfortunately, query expansion with T0++ slightly decreases nDCG@5 performance on average by about 3\,p.p. for relevance judgments and 2\,p.p. for quality judgments. Stance-based re-ranking, however, can increase nDCG@5 effectiveness by up to 5\,p.p. for relevance judgments and by 4\,p.p. for quality judgments. None of our re-ranking stages can sufficiently compensate for the worse retrieval performance of the initial query likelihood ranking.

\input{table-results-stance}
For stance classification, we compare the T0++-based stance classification approach with the best competing team's approach~(Captain Levi, Ro\Bert{}a without fine-tuning) and the baseline~(Puss in Boots) that predicts the majority class~(no stance)
\redtext{NEEDS ACTUAL RESULTS}

\subsection{Transferring Relevance Judgements from Touché~2020--2021}
\label{transfer-relevance-judgements}

We also experiment with transferring the relevance judgements from previous editions of the Touché shared task~\cite{BondarenkoFBGAPBSWPH2020,BondarenkoGFBAPBSWPH2021} in order to increase the amount of judgments to test against.
In~2020 and~2021, however, the results were judged on the document level, not on the passage level as in this year's edition.
To make use of the document-level judgments from Touché~2020--2021, we retrieve passages from this year's extracted passage dataset~\cite{BondarenkoFKSGBPBSWPH2022} but for the topics from previous editions.
We then assume that if a document was relevant all passages extracted from that document are relevant as well, that is, the passages inherit their containing document's relevance label.
Unfortunately, we could only match 16.76\,\% of the top-100 passages retrieved by our baseline with document-level relevance labels from Touché~2020--2021.
Because of the insufficient label coverage we do not include a more detailed evaluation of effectiveness using previous editions' judgements.
