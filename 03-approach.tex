\section{Approach}\label{approach}

\input{figure-pipeline.tex}

We design the architecture of our argumentative retrieval system as a multi-step pipeline that subsequently (re-)ranks, annotates, or modifies documents retrieved for each query with the query likelihood with Dirichlet smoothing~(\(\mu = 1\;000\)). As shown in Figure~\ref{figure-pipeline}, our proposed pipeline consists of four main steps:
\Ni query expansion, reformulation, and combination,
\Nii first-stage retrieval,
\Niii argument quality estimation and stance detection,
and \Niv axiomatic re-ranking and stance-based re-ranking.

\subsection{Query Expansion, Reformulation, and Combination}
\label{reformulation}

The first step of our retrieval pipeline is original query (task's topic titles) reformulation and expansion that aims for increasing a recall.
For that, we use two different strategies: \Ni replacing the comparison objects with their synonyms (e.g., Ubuntu vs.\ Windows $\rightarrow$ Linux vs.\ Windows) and \Nii generating additional, new queries exploiting the topics' description and narrative provided by the task organizers~\cite{BondarenkoFKSGBPBSWPH2022}.
%Expanding the original query with synonyms of comparative objects is motivated by the fact that documents often contain more specific comparisons~(e.g., Ubuntu vs Windows) instead of more broad comparisons~(e.g., Linux vs Windows).
%Yet, specific examples of a more general class of objects are useful to answer comparative questions about their object class.
%We might therefore find relevant documents that would otherwise not match any original query term.
We then address the precision-recall trade-off by deploying re-ranking steps by moving more relevant documents at the top of the ranking~(cf.\ Section~\ref{reranking}).

\paragraph{Query Reformulation with Synonyms.}

To find synonyms of comparison objects mentioned in questions (search queries), we use two different strategies: \Ni word embeddings and \Nii a zero-shot generation with pre-trained large language models.
For the first strategy, we use fastText word embeddings~\cite{BojanowskiGJM2017} from PyMagnitude\footnote{\url{https://gitlab.com/Plasticity/magnitude}} to find words with the highest cosine similarity to the given comparison objects in the embedding space.
We manually examine synonyms from the fastText embeddings pre-trained on different corpora (e.g., Wikipedia and Twitter) and find that the Twitter-based embeddings provide more accurate synonyms.

Our second strategy is based on the T0++~zero-shot language model~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021}.
We prompt the model to generate an answer to the following question: \query{What are synonyms of the word~<token>?}, where \query{<token>}~is one of the two original comparison objects.
We then process the output by splitting by commas and select the first term that is different from the original query term.
With the synonyms returned by either strategy, we replace the comparison objects to formulate new question queries.

\paragraph{Query Reformulation with Topic Context.}
\input{table-generated-queries.tex}

In the next step, we complement the expanded queries with two newly generated ones per topic taking into account the contextual information from the topic's descriptions and narratives that
contain important details on the actual information.
Using the Hugging Face Inference API~\cite{WolfDSCDMCRLFDSPMJPXSGDLR2020}, we prompt T0++ with the following task: \query{<text>. Extract a natural search query from this description.}, where \query{<text>}~is either the topic's narrative or description.
In Table~\ref{table-generated-queries}, we show the examples of generated queries.
Albeit some of the generated queries~(e.g., topic~53) are just reformulations of the original one, T0++ also generates potentially useful meaningful new queries~(e.g., topic~12).

\paragraph{Query Combination and Expansion.}

Finally, we combine up to 5~question queries~(reformulated with synonyms, generated, and the original one, depending on the submitted run; cf.\ Section~\ref{runs}) using a logical disjunction~(Pyserini's \texttt{OR} operator).
We choose the logical disjunction with the outlook on increasing the system's recall and decreasing the chance of empty result sets in the case that search terms are not present in the corpus.

In total we submitted 5~runs (retrieval results;  cf.\ Section~\ref{runs}) to the task, in some of which we use only the original query, and the expanded queries in the others to test the influence of the query expansion and reformulation on the final ranking results.

\subsection{Passage Retrieval}\label{retrieval}

To retrieve passages from the task's corpus, we first build an inverted index using the Pyserini framework~\cite{LinMLYPN2021}.
In the index, we store index term positions, passage vectors, and raw passage contents.
Index terms are stemmed using the Porter stemmer~\cite{Porter1980} and stop words are removed as per the default Pyserini stopword list~\cite{LinMLYPN2021}.
We then retrieve passages for the previously combined query~(cf.\ Section~\ref{reformulation}) using the query likelihood model with Dirichlet smoothing~(\( \mu = 1000 \)).
From this first-stage ranker, we retrieve 100~candidate passages for each query.

\subsection{Argument Tagging, Argument Quality and Stance Classification}
\label{argument-tagging}

After retrieving candidate passages, we tag the argumentative structure (premises and claims), estimate argument quality, and detect the stance (whether the passage is pro first comparison object, pro second, has neutral, or no stance.).
This information is used in later steps of our retrieval pipeline for re-ranking (cf.\ Section~\ref{reranking}).
%Also, the task organizers ask the participants to optionally return a stance for each retrieved document as a sub-task in the Touch√© Lab on Argument Retrieval.
We tag each passage's argumentative structure with the TARGER argument tagger~\cite{ChernodubOHBHBP2019} using the \textttsmall{targer-api} Python package\footnote{\url{https://github.com/webis-de/targer-api}}.

To estimate the passage's argument quality and detect the stance, we first split each passage into sentences using the NLTK library~\cite{BirdLK2009}.
Then each sentence is treated as one potential argument; the quality score and stance for the whole passage is calculated by averaging the quality or stance scores for all sentences in the passage.

\paragraph{Argument Quality Estimation.}
\input{table-quality-stance-mapping.tex}

We use two different methods for assessing the argument quality.
Our first method is based on the IBM Debater API~\cite{ToledoGCFVLJAS2019}.%
\footnote{\url{https://early-access-program.debater.res.ibm.com}}
The API then determines how good the quality of each argument with regard to the topic is with a \Bert-based~\cite{DevlinCLT2019} regression classifier model trained on the IBM-ArgQ-6.3kArgs dataset. The API returns a quality score ranging from~0 (low quality) to~1 (high quality).

As a second method to obtain the argument quality we also use the T0++ model~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021} and prompt it to generate a text to the following task: \query{<sentence>. How would you rate the readability and consistency in this sentence?}\hspace{.5em} \query{very good, good, bad, very bad}, where \query{<sentence>}~is one of the passage sentences.
%This results in an output of either \query{very good}, \query{good}, \query{bad}, or \query{very bad} depending on how the pretrained T0++~model interprets the sentence's argument quality.
We then map the models textual outputs to numeric values using the mapping shown in Table~\ref{table-quality-mapping}.

\paragraph{Stance Detection.}

Stance detection for each sentence uses the same conceptual approaches but with different inputs and outputs.
Since both the IBM Debater API~\cite{BarHaimBDSS2017} and  T0++~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021} can predict only a single-target stance (i.e., for one of the two comparison objects), we combine the two single-target stance scores into a multi-target stance by taking the difference between the stance towards the first object and the stance towards the second object.
We also experimented with different thresholds for the minimal difference between the single-target stances and found a threshold of~0.125 to work well by manually examining some classified examples.

For scoring the single-target argument stance for a sentence with the IBM Debater API, we again query the API with a sentence (argument) and a topic created using one of the comparison objects
The classifier \cite{BarHaimBDSS2017} then computes an argument's likelihood of being pro, con, or neutral with respect to the topic (i.e., the comparison object in our pipeline) by first classifying a sentiment and then detecting whether the topic's and argument's targets contradict each other.
The API then returns a score from from~-1 (against the comparison object) to~+1 (in favor).
By classifying different topics for each object~(i.e., \query{<object>~is good} and \query{<object>~is the best}), we determine an averaged single-target stance for each comparison object.

When using the T0++ for the stance detection, we first experiment with directly prompting the model to output `pro', `con',  or `neutral' labels for the comparison objects.
%However, we found that the T0++~model would not be able to reliably distinguish between a positive and negative stance towards the comparative object.
We formulate the task as two simple questions passed to the model, one to determine whether the sentence has a positive stance towards the comparison object and one to determine whether it has a negative stance: \query{<sentence> Is this sentence pro <object>? yes or no} and \query{<sentence> Is this sentence against <object>? yes or no}, where \query{<sentence>}~is one sentence of the passage and \query{<object>}~is one of the comparative objects.
This results in two answers~(\query{yes} or~\query{no}) for the positive and negative stance respectively. We combine the two textual answers using the mapping shown in Table~\ref{table-stance-mapping}.

\subsection{Axiomatic Re-ranker}
\label{reranking}
\input{table-axioms.tex}

Since recall of our retrieval system is increased by expanding and reformulating queries~(cf.\ Section~\ref{reformulation}), we seek to improve precision by re-ranking the top-10 passages from the first-stage retrieval~(cf.\ Section~\ref{retrieval}) using two different strategies that should rank more argumentative and of higher quality passages also ensuring a balanced overview of the two comparison objects. \Ni We re-rank based on argumentativeness axioms, and \Nii we re-rank based on the passages' stances towards the comparison objects.

\paragraph{Argumentative Axiomatic Re-ranking.}

Ranking methods such as BM25 or query likelihood with Dirichlet smoothing do not capture the ``argumentativeness'' in text that is important for argument retrieval~\cite{BondarenkoFKSGBPBSWPH2022}.
Some approaches for at the TREC Common Core and Decision tracks exploit task-specific, argumentativeness axioms to address the document argumentativeness~\cite{BondarenkoHVSPB2018,BondarenkoFKHVS2019}.
Axioms are constraints that define pairwise ranking preferences between documents or passages.
Because of the promising development in the field of axiomatic information retrieval~\cite{BondarenkoFRSVH2022}, we re-rank the top-10 initially retrieved passages with the \KwikSort algorithm~\cite{HagenVGS2016}.
For axiomatic re-ranking, we compute preferences for 7~argumentativeness axioms specified in Table~\ref{table-axioms}.
The axioms cover general argumentativeness~(ArgUC), argumentative relevance~(QTArg, QTPArg), comparative relevance~(CompArg, CompPArg), and rhetorical and argumentative quality~(aSLDoc, ArgQ).
We then combine the axioms in a majority voting scheme, i.e., we only keep preferences where at least~50\,\% of the 7~axioms agree, and fall back to the original ranking order if less than~50\,\% of all axioms agree.
Using the \iraxioms framework~\cite{BondarenkoFRSVH2022},%
\footnote{\url{https://github.com/webis-de/ir_axioms/}} we then re-rank with the combined axiom.

\paragraph{Stance-based Re-ranking.}

We also implement a stance-based re-ranker to produce rankings where the two conflicting stances (pro first comparison object and pro second comparison object) are nearly equally present.
For balancing the stances, we experiment with two different re-ranking strategies: \Ni alternating stance and \Nii balanced top-\(k\) stance.
For the alternating stance strategy, we split the result set into three lists: \Ni with arguments in favor of the first comparison object, \Nii in favor of the second comparison object, and \Niii neutral arguments or arguments with no stance.
We then alternately select passages from the first two lists. If one or both lists are empty, we fall back to the neutral list.
The balanced top-\(k\) stance strategy is based on the original ranking.
Here we count the number of passages in favor of the first comparison object and the second comparison object in the top-\(k\) initially retrieved passages.
If the difference of these two values is greater than~1, we move the last passage from the majority within the top-\(k\) ranking behind the first minority passage after the top-\(k\) ranking.
This way, passages of the underrepresented stance advance the ranking until the ranking is balanced in the top-\(k\) positions.
In initial experiments, however, we find the alternating stance strategy to be more promising, because the balanced top-\(k\) stance strategy often lead to rankings containing mostly neutral passages.
