\section{Approach}\label{approach}

\input{figure-pipeline.tex}

We design the architecture of our argumentative retrieval system as a multi-step pipeline that subsequently (re-)ranks, annotates, or modifies documents retrieved for each query with the query likelihood with Dirichlet smoothing~(\(\mu = 1\;000\)). As shown in Figure~\ref{figure-pipeline}, our proposed pipeline consists of four main steps:
\Ni query expansion, reformulation, and combination,
\Nii first-stage retrieval,
\Niii argument quality estimation and stance detection,
and \Niv axiomatic re-ranking and fairness re-ranking.

\subsection{Query Expansion, Reformulation, and Combination}
\label{reformulation}

In order to increase recall of our first-stage ranker and to include results for very similar yet differently named objects, we first expand and reformulate the original search query.
Our approaches use two different strategies: \Ni we replace the comparative objects with their synonyms and \Nii we generate additional, new queries using the additional description and narrative information provided by the shared task organizers~\cite{BondarenkoFKSGBPBSWPH2022}.
Expanding the original query with synonyms of comparative objects is motivated by the fact that documents often contain more specific comparisons~(e.g., Ubuntu vs Windows) instead of more broad comparisons~(e.g., Linux vs Windows).
Yet, specific examples of a more general class of objects are useful to answer comparative questions about their object class.
We might therefore find relevant documents that would otherwise not match any original query term.
It is however important to note that increasing recall can result in an decrease in precision which is undesirable in the precision-oriented setting of the shared task.
However, we later apply re-ranking steps that improve precision by moving irrelevent documents further down the ranking~(c.f. Section~\ref{reranking}).

\paragraph{Query Expansion with Synonyms}

We use two different strategies to find synonyms: \Ni word embeddings and \Nii a zero-shot language model.
In our first strategy, we use fastText word embeddings~\cite{BojanowskiGJM2017} to find words with the highest cosine similarity to the given comparative objects in the embedding space.
We manually examine synonyms from fastText embeddings using different domains~(i.e., Wikipedia and Twitter) and find that fastText embeddings trained on the Twitter corpus result in the best synonyms.

Our second strategy to obtain synonyms is based on the T0++~zero-shot language model~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021}.
We ask the model to generate an answer to the following task: \query{What are synonyms of the word~<token>?} where \query{<token>}~is one of the two comparative objects.
From the text returned by the language model, we then parse synonyms by splitting at commas and remove duplicate synonyms.
With the synonyms returned by either strategy, we replace the comparative objects to form new queries. All alternative queries and the original query are then combined.

\paragraph{Query Reformulation with Topic Context}
\input{table-generated-queries.tex}

We complement the queries expanded by replacing synonyms with newly generated queries that incorporate the contextual information provided in description narrative fields from the shared task's topics.
The description contains important details about the actual information need and the narrative clearly defines which passages are relevant for the query.
We use this valuable information about which passages to retrieve by generating new queries with the T0++~language model~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021} using the Huggingface Inference API~\cite{WolfDSCDMCRLFDSPMJPXSGDLR2020} and providing it with a topic's description or narrative.

We challenge the T0++~model with the following task: \query{<text>. Extract a natural search query from this description.} where \query{<text>}~is either the topic's narrative or description.
The string returned by the language model is then used as is as the new query for the topic and combined with the previous queries.
In Table~\ref{table-generated-queries}, we show examples of generated queries.
Albeit some of the generated queries~(e.g., topic~53) are just reformulations of the original query, the T0++~language model generates some interesting new queries for other topics~(e.g., topic~12).

\paragraph{Disjunctive Query Combination}
After the query expansion and query reformulation steps, we need to combine all computed queries and the original query.
We decide to combine all queries in a single query in a logical disjunction, that is by using Pyserini's OR operator.
Two reasons influence this decision:
Firstly, retrieving results for just one query is conceptually easier as we don't need to interleave multiple result sets after the retrieval step.
Interleaving is not trivial and it is often challenging to find an interleaving strategy without many caveats.
Secondly, the logical disjunction increases the system's recall and decrease the chance of empty result sets in the case that a term is not present in the corpus.
Although, the query reformulation, expansion and combination steps are optional, meaning that we use these steps only in some runs. In most of our submitted runs, we just use the original query, because the increase in recall might result in a decrease query in precision that is hard to offset in subsequent re-ranking steps.

\subsection{Passages Retrieval}\label{retrieval}

To retrieve passages from the set of passages extracted from ClueWeb~12 by \citet{BondarenkoFKSGBPBSWPH2022}, we first build an inverted index using the Pyserini framework~\cite{LinMLYPN2021}.
Pyserini allows for experimenting with multiple steps of a retrieval system including indexing and simple retrieval models like Okapi~BM25 or the query likelihood model.
In the index, we store index term positions, passage vectors, and raw passage contents.
Index terms are stemmed using the Porter stemmer~\cite{Porter1980} and stop words are removed as per the default Pyserini stopword list~\cite{LinMLYPN2021}.
We then retrieve passages for the previously combined query~(c.f. Section~\ref{reformulation}) using the query likelihood model with Dirichlet smoothing~(\( \mu = 1000 \)) in Pyserini.
From this first-stage ranker, we retrieve 100~candidate passages for each query.

\subsection{Argument Quality and Stance Tagging}
\label{argument-tagging}

After retrieving candidate passages, we tag the argumentative structure, argumentative quality and argument stance.
Argument structure, quality, and stance are required for later steps in our retrieval pipeline to re-rank the passages~(c.f. Section~\ref{reranking}).
Also, the task organizers ask the participants to optionally return a stance for each retrieved document as a sub-task in the Touch√© Lab on Argument Retrieval.
We tag each passage's argumentative structure with the TARGER argument tagger~\cite{ChernodubOHBHBP2019} using the \textttsmall{targer-api} Python package\footnote{\url{https://github.com/webis-de/targer-api}}.
In order to tag each passage's quality and stance we first split each retrieved passage into sentences using the NLTK library~\cite{BirdLK2009}.
Then each sentence is treated as one potential argument and tagged with its argumentative quality and stance.
To find the quality score and stance for the whole passage, we average the quality or stance scores respectively of all sentences in the passage.

\paragraph{Argument Quality Tagging}
\input{table-quality-stance-mapping.tex}

We implement two different methods for quality tagging:
Our first method is based on the IBM Debater API~\footnote{\url{https://early-access-program.debater.res.ibm.com}}~\cite{ToledoGCFVLJAS2019}.
Here we send each sentence from one passage and the original query as the topic to the IBM Debater API for argument quality assessment. of Passages
The API then determines how good the quality of each argument with regards to the topic is with a \Bert-based~\cite{DevlinCLT2019} regression classifier model trained on the IBM-ArgQ-6.3kArgs dataset. The model and therefore the API then returns a quality score ranging from 0 to 1 where a classified score of~0 means very poor argument quality and a score of~1 means very good argument quality~\cite{ToledoGCFVLJAS2019}.

As a second method to obtain the argument quality we also use the T0++~language model~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021}.
We ask the zero-shot language model to generate a text the following task: \query{<sentence>. How would you rate the readability and consistency in this sentence?}\hspace{1.5em} \query{very good, good, bad, very bad} where \query{<sentence>}~is one sentence of a passage.
This results in an output of either \query{very good}, \query{good}, \query{bad}, or \query{very bad} depending on how the pretrained T0++~model interprets the sentence's argument quality.
We then map this textual output labels to numeric values as per the mapping shown in Table~\ref{table-quality-mapping}.

\paragraph{Argument Stance Tagging}

Stance detection for each sentence uses the same conceptual approaches but with different inputs and outputs.
Since both the IBM Debater API~\cite{BarHaimBDSS2017} and our T0++~approach~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021} are only capable of calculating a single-target stance~(i.e., for one of the two comparative objects), we combine the two single-target stances into a multi-target stance by taking the difference between the stance towards the first comparative object and the stance towards the second comparative object.
We also experimented with different thresholds for the minimal difference between the single-target stances and found a threshold of~0.125 to work well when manually examining some classified examples.

For scoring the single-target argument stance for a sentence with the IBM Debater API, we again send the sentence and a claim built using one of the comparative objects to the IBM Debater API.
The classifier by \citet{BarHaimBDSS2017} computes an argument's likelihood of being pro, con, or neutral with respect to the claim~(i.e., the comparative object in our pipeline) by first classifying sentiments and then detecting contrasts in the topic and argument claim targets.
The API then returns a score from from~-1 to~+1 where -1~means the argument is against the comparative object and +1~means that the argument is in favor of the comparative object.
By classifying different claims for each object~(i.e., \query{<object>~is good} and \query{<object>~is the best}), we get an averaged single-target stance for each comparative object.

For the second method using the T0++~language model~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021} we first experiment with directly asking the model to generate~\query{pro}, \query{con}~or~\query{neutral} classification labels for a comparative object.
However, we found that the T0++~model would not be able to reliably distinguish between a positive and negative stance towards the comparative object.
Therefore we instead reformulate the task in two simple questions, one to determine whether the sentence has a positive stance towards the comparative object and one to determine whether it has a negative stance: \query{<sentence> Is this sentence pro <object>? yes or no} and \query{<sentence> Is this sentence against <object>? yes or no} where \query{<sentence>}~is one sentence of the passage and \query{<object>}~is one of the comparative objects.
This results in two answers~(\query{yes} or~\query{no}) for the positive and negative stance respectively. We combine the two textual answers as shown in Table~\ref{table-stance-mapping} and then combine the single-target stances into a multi-target stance as described above.

\subsection{Axiomatic Re-ranker}
\label{reranking}
\input{table-axioms.tex}

Because we increase the recall of our retrieval system by expanding and reformulating queries~(c.f. Section~\ref{reformulation}), we re-rank top-10 result passages of our first-stage query likelihood retrieval~(c.f. Section~\ref{retrieval}).
We seek to improve our system's precision by re-ranking with two different strategies that should rank passages higher that are more argumentative and higher quality, but also ensure a fair overview of the two conflicting comparative objects: \Ni we re-rank based on argumentative retrieval axioms and \Nii we re-rank based on the passages' stances towards the comparative objects.

\paragraph{Argumentative Axiomatic Re-ranking}

When only using frequency-based ranking methods such as BM25 or query likelihood with Dirichlet smoothing, it is difficult to capture the inherent argumentativeness in passages.
This argumentativeness, however, is most important for finding relevant and useful opinions on comparative questions~\cite{BondarenkoFKSGBPBSWPH2022}.
Also, passages in our pipeline are already annotated with argumentative quality and stance.
Recent approaches for the TREC Common Core and Decision tracks exploit task-specific, argumentative retrieval axioms to ensure argumentativeness in their results~\cite{BondarenkoHVSPB2018,BondarenkoFKHVS2019}.
Axioms are constraints that define pairwise preferences between documents or passages.
Because of the promising development in the field of axiomatic information retrieval~\cite{BondarenkoFRSVH2022}, we re-rank the top-10 passages with the \KwikSort algorithm~\cite{HagenVGS2016}.
For axiomatic re-ranking, we compute preferences for 7~argumentative axioms listed in Table~\ref{table-axioms}.
The axioms cover general argumentativeness~(ArgUC), argumentative relevance~(QTArg, QTPArg), comparative relevance~(CompArg, CompPArg), and rhetorical and argumentative quality~(aSLDoc, ArgQ).
We then combine the axioms in a majority voting scheme, i.e., we only keep preferences where at least~50\,\% of the 7~axioms agree, and fall back to the original ranking order if less than~50\,\% of all axioms agree.
Using the \iraxioms framework~\cite{BondarenkoFRSVH2022},%
\footnote{\url{https://github.com/webis-de/ir_axioms/}} we then re-rank with the combined axiom.

\paragraph{Fairness Re-ranking}

We also implement a fairness re-ranker to produce rankings where the two conflicting stances~(pro first compared object and pro second compared object) are nearly equally prominent.
For balancing the argumentative stances, we experiment with two different re-ranking strategies: \Ni alternating stance and \Nii balanced top-\(k\) stance.
For the alternating stance strategy, we split the result set into three lists: First, with arguments in favor of the first comparative object, second, in favor of the second comparative object, and last, neutral arguments or arguments with no stance.
We then alternately select passages from the first two lists. If one or both lists are empty, we fall back to the neutral list.
The balanced top-\(k\) stance strategy is based on the original ranking.
Here we count the number of passages in favor of the first comparative object and the second comparative object in the top-\(k\) result set.
If the two numbers are imbalanced, i.e., their difference is greater than~1, we move the last passage from the majority within the top-\(k\) ranking behind the first minority passage after the top-\(k\) ranking.
This way, passages of the underrepresented stance advance the ranking until the ranking is balanced in the top-\(k\) positions.
In initial experiments, however, we find the alternating stance strategy to be more promising, because the balanced top-\(k\) stance strategy often lead to rankings containing mostly neutral passages.
