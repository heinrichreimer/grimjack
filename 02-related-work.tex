\section{Related Work}

Personal decision making often starts with formulating comparative questions like ``Should I major in philosophy or psychology?''~\cite{BondarenkoFBGAPBSWPH2020,BondarenkoGFBAPBSWPH2021,BondarenkoFKSGBPBSWPH2022}. Short direct answers (potentially biased)~\cite{PotthastHS2020} to such questions might be insufficient; instead, they require diverse opinions to provide a sufficient, balanced, and argumentative overview~\cite{BondarenkoFBGAPBSWPH2020}.
The Touché shared task on Argument Retrieval for Comparative Questions was proposed to evaluate retrieval approaches on the large corpus with respect to relevance and rhetorical quality of potential answers to comparative questions that also may represent different standpoints~\cite{BondarenkoADHBH22}.

The most effective approaches at previous Touché editions~\cite{BondarenkoFBGAPBSWPH2020,BondarenkoGFBAPBSWPH2021} successfully used query expansion with synonyms and antonyms~\cite{AbyeST2020}, identified premises and claims in retrieved documents~\cite{Huck2020, ShirshakovaW2021}, estimated argument quality~\cite{AbyeST2020}, and re-ranked initially retrieved documents based on argument quality and document ``comparativeness'', e.g., a ratio of comparative adjectives~\cite{ChekalinaP2021}. Inspired by the participant approaches from the previous Touch{\'e} editions, we also include the components of argument mining and argument quality estimation in our retrieval pipeline, however, using different methods.
We rely on a large language model~T0 trained in multitask setting that showed to achieve state-of-the-art results for various Natural Language Processing tasks in zero-shot settings~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021}. The largest pretrained T0 variant, T0++, was trained on 62~datasets with 12~task-specific prompts covering such tasks as question answering, sentiment analysis, summarization, etc. By using T0++, we aim for answering a question whether the abilities of large language models are sufficient for the new task of argument retrieval.

Our second idea of axiomatic re-ranking comes from axiomatic thinking in information retrieval, where axioms formally describe constraints that good retrieval model should fulfil, e.g., documents with more query term occurrences should be ranked higher~\citet{FangTZ2004}. It has already been shown that combining multiple axioms for re-ranking results of arbitrary retrieval models can improve final overall retrieval effectiveness~\cite{HagenVGS2016}. Complementing existing retrieval axioms, \citet{BondarenkoHVSPB2018} introduced argument axioms based on claims and premises in documents identified with TARGER~\cite{ChernodubOHBHBP2019}.
%The \iraxioms\footnote{\url{https://github.com/webis-de/ir_axioms/}} Python framework facilitates defining own task-specific axioms, combining multiple axioms, and using axioms for re-ranking using the \KwikSort algorithm~\cite{BondarenkoFRSVH2022,HagenVGS2016} and thus opens the opportunity to propose new argumentative axioms that exploit a document's ``comparativeness'' or argumentative quality.

%At the Touché shared tasks, living and reproducible software is favored over generated run files.
%Using the TIRA platform\footnote{\url{https://tira.io}}~\cite{PotthastGWS2019}, scientific approaches, systems and models can be deployed in virtual machines in order to ease reproducing their results~\cite{PotthastGWS2019}.
%Additionally, the TIRA system automatically evaluates submitted approaches of shared task participants and reports a reproducible leaderboard of the \nDCG{5} scores.\footnote{\url{https://tira.io/task/touche-task-2/dataset/touche-2022-task2}}
