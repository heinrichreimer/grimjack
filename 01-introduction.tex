\section{Introduction}\label{intro}

%Current information retrieval systems are well able to answer keyword queries or simple questions but answering comparative questions is still a challenging task~\cite{BondarenkoGFBAPBSWPH2021}.
Argument retrieval is a specific task that not only considers topical relevance of retrieved documents to given queries (usually of controversial, argumentative or opinion nature) but also accounts for argument specific features like argument quality and stance~\cite{BondarenkoFBGAPBSWPH2020, BondarenkoGFBAPBSWPH2021}.  
Furthermore, it has been shown that current search engines might return biased results~\cite{ShahB2022} and argument retrieval systems return ``unfairelly'' distributed pro / con arguments~\cite{CherumanalSSC2021}. We especially emphasize the importance of retrieving diverse results for comparative questions (e.g., ``Train or plane? Which is the better choice?'') that provide different point of views to mitigate biasing users' decisions towards one or the other comparison option.

%\citet{BondarenkoFKSGBPBSWPH2022} organize the Touché Lab on Argument Retrieval at CLEF to find different promising approaches that tackle the task of retrieving arguments to comparative questions from a web-scale collection of passages.
%The retrieved arguments should be of high rhetorical an argumentative quality but also relevant to the query.
%Additionally, in~2022 the participants of the Touché shared task should also tag their resulting passages with their stance towards the comparative objects.
%Answering comparative questions supports users in their everyday lives but users might struggle with converting those natural language questions into keyword queries~\cite{BondarenkoGFBAPBSWPH2021}.

Our Team Grimjack participated in the Touch{\'e} shared task on Argument Retrieval for Comparative Questions which goals are: \Ni To retrieve relevant and high quality argumentative passages from a collection of 0.9~million text passages to a set of 50~search topics and \Nii to classify the stance of the retrieved passages towards the comparison objects in search topics~\cite{BondarenkoFKSGBPBSWPH2022}. As part of our participation in the task, we have developed a flexible retrieval pipeline in Python based on Pyserini~\cite{LinMLYPN2021} as an easily configurable command line application. In the first step, our approach uses query (comparative questions from topics' titles) reformulation and expansion by important terms from topics descriptions and narratives. Then the top~\todo{X} initially retrieved passages using query likelihood with Dirichlet smoothing~\todocite are axiomatically re-ranked based on the \todo{number} of premises and claims \todo{identified with} and argument quality predictions by the \todo{classifier}. Finally, the pro and con argumentative passages towards the comparative objects are balanced in the final ranking by \todo{method} (cf.\ Section~\ref{approach} for more details on the approach and submitted runs).

Additionally, we manually labeled the \todo{relevance} of \todo{XX} documents for three topics returned by our system (before the official results are made available by the organizers). Our manual assessment shows the potential of expanding queries with synonyms and contextual information for improving the effectiveness of our argument retrieval approach~(cf.\ Section~\ref{evaluation}).

%We develop a flexible retrieval pipeline to participate in the Touché shared task on argument retrieval for comparative questions~(c.f. Section~\ref{approach}).
%Our approach is developed with Python and Pyserini~\cite{LinMLYPN2021} as an easily configurable command line application.
%With query reformulation, expansion and combination we relax the initial query and exploit additional topic context such as description and narrative.
%After retrieving candidate passages using query likelihood with Dirichlet smoothing, we tag each passage's argumentative structure, argument quality and the stance towards the comparative objects.
%To improve relevance and quality on high ranks, we then re-rank results axiomatically using specialized argumentative retrieval axioms, incorporating the previously annotated argument quality.

%Two strategies to balance the passages' stances towards the comparative objects showcase fair re-ranking for arguments.

%Even though we were unable to transfer enough relevance judgements from previous editions of the Touché shared tasks, our manual assessment of three topics shows the potential of expanding queries with synonyms and contextual information~(c.f. Section~\ref{evaluation}).

%% The last statement could fit in the CR version depending on the final evaluation: good or bad
%Different configurations for our submitted runs should pose examples to discuss current doubts about the usefulness of large zero-shot language models like T0++~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021} in the field of argumentative information retrieval~\cite{ShahB2022}.
