\section{Introduction}\label{intro}

Argument retrieval is a specific task that not only considers topical relevance of retrieved documents to given queries (usually of controversial, argumentative or opinion nature) but also accounts for argument specific features like argument quality and stance~\cite{BondarenkoFBGAPBSWPH2020, BondarenkoGFBAPBSWPH2021}.  
Furthermore, it has been shown that current search engines might return biased results~\cite{ShahB2022} and argument retrieval systems return ``unfairelly'' distributed pro / con arguments~\cite{CherumanalSSC2021}.
We especially emphasize the importance of retrieving diverse results for comparative questions (e.g., ``Train or plane? Which is the better choice?'') that provide different point of views to mitigate biasing users' decisions towards one or the other comparison option.

Our Team Grimjack participated in the Touch{\'e} shared task on Argument Retrieval for Comparative Questions which goals are: \Ni To retrieve relevant and high quality argumentative passages from a collection of 868\,655~text passages to a set of 50~search topics and \Nii to classify the stance of the retrieved passages towards the comparison objects in search topics~\cite{BondarenkoFKSGBPBSWPH2022}.
As part of our participation in the task, we have developed a flexible retrieval pipeline in Python based on Pyserini~\cite{LinMLYPN2021} as an easily configurable command line application.
In the first step, our approach uses query (comparative questions from topics' titles) reformulation and expansion by important terms from topics' descriptions and narratives. Then the top~100 initially retrieved passages using query likelihood with Dirichlet smoothing~\cite{ZhaiL2001} are axiomatically re-ranked based on the number and position of premises, claims, and comparative objects (identified with TARGER~\cite{ChernodubOHBHBP2019}) and argument quality predictions by the IBM Debater API~\cite{ToledoGCFVLJAS2019} and T0++~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021}.
Finally, the pro and con argumentative passages towards the comparative objects are balanced in the final ranking by alternating documents of different stance (cf.\ Section~\ref{approach} for more details on the approach and submitted runs).

Additionally, we manually labeled the relevance and stance of 35~documents for three topics returned by our system (before the official results are made available by the organizers). Our manual assessment shows that depending on the query our T0-based query expansion or applying argumentative axioms and fair re-ranking can improve the effectiveness for different queries~(cf.\ Section~\ref{evaluation}).

%% The last statement could fit in the CR version depending on the final evaluation: good or bad
%Different configurations for our submitted runs should pose examples to discuss current doubts about the usefulness of large zero-shot language models like T0++~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021} in the field of argumentative information retrieval~\cite{ShahB2022}.
