\section{Submitted Runs}
\label{runs}

%For fast development and evaluation of different parameter settings, we complement our modular retrieval pipeline~(cf., Section~\refname{approach}) with an evaluation component. Our evaluation module automatically loads relevance judgments from previous editions of the Touch√© shared tasks on Argument Retrieval~(c.f. Section~\ref{transfer-relevance-judgements}) and transfers the document-level relevance judgments from~2020 and~2021 to the passages from the 2022 dataset.

We submit five runs that use different components and strategies of our pipeline~(cf.\ Section~\ref{approach}) to the Touch{\'e} second task.
Instead of uploading generated run files, we deploy our retrieval system as a working software on the TIRA platform~\cite{PotthastGWS2019}.

\paragraph{Query Likelihood Baseline (Run 1).}

For our first run, we simply retrieve top-100 passages ranked by query likelihood with Dirichlet smoothing~\cite{ZhaiL2001} (\(\mu = 1000\)) for the original, unmodified queries (topic titles) and tag argument stance by comparing sentiments for each object using the IBM Debater API, treating a stance under a threshold of~0.125 as neutral.

\paragraph{Argument Axioms (Run 2).}

To produce our second run, we re-rank the top-10 passages from the baseline result using \KwikSort~\cite{BondarenkoFRSVH2022,HagenVGS2016} based on preferences from the argument axioms as described in Section~\ref{reranking}.

\paragraph{Stance-based Re-ranking with Argumentative Axioms (Run 3).}

Our third run also uses argument axiomatic re-ranking after the baseline retrieval. However to ensure that the stances towards both comparison objects are nearly equally represented in the result ranking, we apply stance-based re-ranking with the alternating stance strategy as described in Section~\ref{reranking}.

\paragraph{All You Need is T0 (Run 4).}

Large language models have recently found application in many NLP tasks, web search, or retrieval. The trend of using large language models for solving almost any task has also been criticized. For instance, \citet{ShahB2022} highlight conceptual flaws that question if such an extreme usage of not fully understood models is desirable when implementing search for answers to real-life questions~(e.g., in search engines).

In our fourth submitted run, we want test a language model's T0++ zero-shot classification abilities. First, we reformulate and generate and combine queries; final queries are an expansion of the topic titles~(cf.\ Section~\ref{reformulation}).
We then retrieve 100~documents using query likelihood, and use T0++ again to estimate argument quality and stance~(cf.\ Section~\ref{argument-tagging}).

\paragraph{Argumentative Stance-based Re-ranking with T0 (Run 5).}

In our last run, we combine most of the methods introduced in Section~\ref{approach} to generate a ranking that is both as argumentative as possible and equally represents argument stances, but also uses T0++ for query reformulation and expansion.
Here, we combine new queries generated by T0++ and reformulate queries by replacing synonyms returned by T0++. However, we also use synonyms from the fastText~\cite{BojanowskiGJM2017} embedding similarity method~(cf.\ Section~\ref{reformulation}); final queries are an expansion of the topic titles.
The top-10 results of the 100 passages retrieved using query likelihood for the expanded queries are then re-ranked based on the argumentativeness axioms and by alternating stance~(cf.\ Section~\ref{reranking}).