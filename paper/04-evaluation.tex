\section{Evaluation} \label{evaluation}
    The approach has been successfully deployed on the evaluation platform Tira\footnote{https://www.tira.io/}. Our approach can be found under the name XXXX. Tira automatically evaluates the submitted approaches of the participants of the shared task and reports the NDCG@5. The organizers use this site to ensure a reproducible leaderboard. We will now have a closer look at the performance of our approach by looking at the first 5 results of three chosen topics for two different runs. The two runs we are looking at are (1) without query reformulation/expansion and axiomatic reranking and (2) with query reformulation/expansion and axiomatic reranking.
    \subsection{Topic 1}
        The first query we will have a look at will be: \textit{Which is better, a laptop or a desktop?} According to the narrative field provided by the organizers' passages that contain major similarities and dissimilarities of laptops and desktops are relevant. Also relevant are passages which contain advantages and disadvantages of specific usage scenarios.
        \subsubsection*{Run 1}
            The first returned passage contains arguments for laptops and also for desktops while being subjective and non-biased. Our second and fifth results are product descriptions of laptops and do not deliver arguments for or against laptops/desktops. The third returned passage shows us results for laptops on a website where we can buy laptops. Our fourth result gives us 5 arguments why laptops are better. So manually we would  change the ranking as follows: 1-4-2-5-3
        \subsubsection*{Run 2}
            Our second run retrieves passages on its first three places which are about different laptops. These websites give a detailed overview of the laptop they are talking about but no argument for or against laptops/desktops is given. On our fourth place, we have a review for a network switch. The fifth place is a selling website for electronic products. We must admit that the first five results are not relevant and do not provide argumentative support to the user. The passage which was our first place in our first run is now in place 34. An explanation might be that using synonyms, in this case, lead to inaccurate search queries which did not capture the user's information need.
    \subsection{Topic 2}
        The next question we will have a look at is: \textit{Is OpenGL better than Direct3D in terms of portability to different platforms?} Relevant passages should contain information about the portability of OpenGL/Direct3D across different operation systems. Not relevant are ads or tutorials for OpenGL/Direct3D.
        \subsubsection*{Run 1}
            The first result talks about 3D-rendering in the Opera browser and does not provide any arguments. The second done is a gaming-related blog post again no arguments are given. The third and fourth results are the same page (but different ClueWeb-ID) and talk about a gaming developer who converted his game from OpenGL to Direct3D. He talks about his problems and gives tips on which pattern to avoid while developing a 3D game. In our last passage, we can get the information that Direct3D is now supported in Linux. The only passages which are kind of relevant are the third and fourth. All other of the first five passages are not relevant. 
        \subsubsection*{Run 2}
            The returned results are the same passages returned by run 1 but in a different order. The first two passages are the ones that talk about the game developer and his problems converting from OpenGL to Direct3D. Then we will get the passage which is about the support of Direct3D in Linux. In fourth place we have the passage talking about the 3D-rendering in Opera and the last result is the gaming-related blog. So the ranking has slightly improved since the most relevant passages are now in place 1 and 2 while other less relevant passages are now further behind in the ranking.
    \subsection{Topic 3}
        Our last topic will be: \textit{Which technology performs better: Apple's or Google's?} Relevant passages should compare both companies in terms of service and their products. Relevant passages can focus on one company. Not relevant passages are about genric information about the companies.
        \subsubsection*{Run 1}
            The first passage returned is a news site about technology and talks about various topics from this domain but no arguments are given. The second passage talks about the new Google TV while AppleTV has been already released. In the third place, there is a passage that talks about how Apple and Google both signed a privacy accord. The fourth result is about Facebook buying Instagram. The last result is about Google being caught violating users' privacy while Apple has been already caught violating users' privacy. So we have some relevant passages here. We would rank the passages as follows: 3-5-2-4-1. So the top-5 ranking is not very good. 
        \subsubsection*{Run 2}
            The first passage is a news site about Apple products and provides some insight into these products. The second passage provides information about selling numbers and stock prices. So this site is not relevant according to the narrative provided by the organizers. The next passage is a news article about how Google and Motorola have to hand over Android information to Apple. So no arguments are provided in this passage. The fourth result is about how Apple could only have success because of Google.  The last passage talks about five reasons iPhone vs Android is not Mac vs Windows. So we can see that different passages have been retrieved for run 1 and run 2. For run 2 we would rank the passages as follows: 1-4-5-3-2.