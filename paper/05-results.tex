\section{Results}
\label{results}

We evaluate our approach by effectiveness to retrieve relevant and high-quality passages and to predict the correct stance towards the comparison objects, using manual judgments provided by Touch{\'e}. The task organizers asked human volunteers to label each document pooled from all submitted runs at depth~5 with respect to relevance~(0: not relevant, 1: relevant, 2: highly relevant), rhetorical quality~(0: low quality or not argumentative, 1: average quality, 2: high quality), and stance~(pro first object, pro second object, neutral, no stance).

The results for the relevance and quality effectiveness using nDCG@5~(Tables~\ref{table-results-relevance} and~\ref{table-results-quality}) show that our baseline Run~1 using query likelihood with Dirichlet smoothing performs worse than the BM25~baseline~(Puss in Boots~\cite{BondarenkoFKSGBPBSWPH2022}). Since our other runs re-rank retrieved results from the initial ranking, we compare our individual re-ranking strategies. Nonetheless, we acknowledge that all of our submitted runs are outperformed by the BM25~baseline and other dense rankers' results submitted to the shared task.
The differences in nDCG@5 scores compared to our query likelihood baseline indicate that axiomatic re-ranking (Run~2) can increase consistency with argumentativeness axioms while retaining equal retrieval effectiveness. Unfortunately, query expansion with T0++ slightly decreases nDCG@5 on average by about 3\,p.p. for relevance judgments and 2\,p.p. for quality judgments. Stance-based re-ranking, however, can increase nDCG@5 by up to 5\,p.p. for relevance judgments and by 4\,p.p. for quality judgments. None of our re-ranking stages could sufficiently compensate for the worse retrieval performance of the initial query likelihood ranking.

\input{table-results-relevance}
\input{table-results-quality}

\input{table-results-stance}
For stance detection, we compare the T0-based stance classification approach with the best competing team's approach~(Captain Levi, pre-trained Ro\Bert{}a without fine-tuning) and the baseline~(Puss in Boots) that predicts the majority class~(`no stance'). In Table~\ref{table-results-stance}, we report a macro-averaged F$_1$-score per run and per team as well as the number of documents~$N$ for which the predicted stance has a ground-truth label as provided by the task organizers. We observe that since only the top-5 passages were pooled for manual judgments only a limited number of predicted stance labels~(e.g. 1208~for Run~4) can be used for evaluation, even though we predicted the stance up to depth~100~(i.e. 5000~predicted stance labels per run). In this setting our Run~4~(i.e. stance prediction using T0++; cf.\ Section~\ref{argument-tagging}) has the highest macro-averaged F$_1$-score of all submitted runs to the task. However, due to the limited number of labels available for evaluation and because the number of available labels differs across teams and runs, we cannot directly compare different runs. For example, the 3\,792~unjudged labels from Run~4 could be correctly predicted~(i.e., increasing F$_1$) or incorrectly predicted~(i.e., decreasing F$_1$). As an alternative, comparable measure, in the rightmost columns of Table~\ref{table-results-stance}, we report F$_1$-scores of predicted stances of only the top-5 passages of each run. All 250~stance labels from the top-5 results of each submitted run have corresponding ground-truth labels due to the organizers' top-5 pooling for manual judgment.
When considering only the top-5 passages, our stance classification approach using T0++ falls behind Team Captain Levi's best performing approaches.
However, 250~samples might also be an insufficient sample size to compare classifier performance. It is also unclear how examining only top results affects the evaluation of classification performance.
