\section{Approach}\label{approach}

\input{figure-pipeline.tex}

We design the architecture of our argumentative retrieval system as a pipeline of multiple steps that subsequently (re-)rank, annotate, or modify the documents or queries given as inputs. This pipeline is shown in Figure~\ref{figure-pipeline}.
We identify four core steps as most important to our approach:
\Ni query expansion, reformulation, and combination,
\Nii first-stage retrieval,
\Niii argument quality and stance tagging,
and \Niv axiomatic reranking and fairness reranking.

Additionally, we add an evaluation component that is not shown in Figure~\ref{figure-pipeline} because it is not needed to retrieve results from our system.
With this evaluation component we can evaluate our system on topics of previous editions~(i.e.,~2021 and~2020) of the Touché Lab on Argument Retrieval~(c.f.~Section~\ref{transfer-relevance-judgements}).

\subsection{Query Expansion, Reformulation, and Combination}
\label{reformulation}

In order to increase recall of our first-stage ranker and to include results for very similar yet differently named objects, we first expand and reformulate the original search query.
Our approaches use two different strategies: \Ni we replace the comparative objects with their synonyms and \Nii we generate additional, new queries using the additional description and narrative information provided by the shared task organizers.
Expanding the original query with synonyms of comparative objects is motivated by the fact that documents often contain more specific comparisons~(e.g., Ubuntu vs Windows) instead of more broad comparisons~(e.g., Linux vs Windows).
Yet, specific examples of a more general class of objects are useful to answer comparative questions about their object class.
We might therefore find relevant documents that would otherwise not match any original query term.
It is however important to note that increasing recall can result in an decrease in precision which is undesirable in the precision-oriented setting of the shared task.
However, we later apply re-ranking steps that improve precision by moving irrelevent documents further down the ranking~(c.f.~Section~\ref{reranking}).

\paragraph{Query Expansion with Synonyms}

We use two different strategies to find synonyms: \Ni word embeddings and \Nii a zero-shot language model.
In our first strategy, we use fastText word embeddings~\cite{BojanowskiGJM2017} to find words with the highest cosine similarity to the given comparative objects in the embedding space.
We manually examine synonyms from fastText embeddings using different domains~(i.e., Wikipedia and Twitter) and find that fastText embeddings trained on the Twitter corpus result in the best synonyms.

Our second strategy to obtain synonyms is based on the T0++~zero-shot language model~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021}.
We ask the model to generate an answer to the following task: \query{What are synonyms of the word~<token>?} where \query{<token>}~is one of the two comparative objects.
From the text returned by the language model, we then parse synonyms by splitting at commas and remove duplicate synonyms.
With the synonyms returned by either strategy, we replace the comparative objects to form new queries. All alternative queries and the original query are then combined.

\paragraph{Query Reformulation with Topic Context}
\input{table-generated-queries.tex}

We complement the queries expanded by replacing synonyms with newly generated queries that incorporate the contextual information provided in description narrative fields from the shared task's topics.
The description contains important details about the actual information need and the narrative clearly defines which passages are relevant for the query.
We use this valuable information about which passages to retrieve by generating new queries with the T0++ language model~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021} and providing it with a topic's description or narrative.

We challenge the T0++ model with the following task: \query{<text>.~Extract a natural search query from this description.} where \query{<text>}~is either the topic's narrative or description.
The string returned by the language model is then used as is as the new query for the topic and combined with the previous queries.
In Table~\ref{table-generated-queries}, we show examples of generated queries.
Albeit some of the generated queries~(e.g., topic~53) are just reformulations of the original query, the T0++ language model gernerates some interesting new queries for other topics~(e.g., topic~12).

\paragraph{Disjunctive Query Combination}
After the query expansion and query reformulation steps, we need to combine all computed queries and the original query.
We decide to combine all queries in a single query in a logical disjunction, that is by using Pyserini's OR operator.
Two reasons influence this decision:
Firstly, retrieving results for just one query is conceptually easier as we don't neet to interleave multiple result sets after the retrieval step.
Interleaving is not trivial and it is often challenging to find an interleaving strategy without many caveats.
Secondly, the logical disjunction increases the system's recall and decrease the chance of empty result sets in the cse that a term is not present in the corpus.
Although, the query reformulation, expansion and combination steps are optional, meaning that we use these steps only in some runs. In most of our submitted runs, we just use the original query, because the increase in recall might result in a decreasequery in precision that is hard to offset in subsequent re-ranking steps.

\subsection{Passages Retrieval}\label{retrieval}

To retrieve passages from the set of passages extracted from ClueWeb~12 by the task organizers, we first build an inverted index using the Pyserini framework~\cite{LinMLYPN2021}.
Pyserini allows for experimenting with multiple steps of a retrieval system including indexing and simple retrieval models like Okapi~BM25 or the query likelihood model.
In the index, we store index term positions, passage vectors, and raw passage contents.
Index terms are stemmed using the Porter stemmer~\cite{Porter1980} and stop words are removed as per the default Pyserini stopword list~\cite{LinMLYPN2021}.
We then retrieve passages for the previously combined query~(c.f. Section~\ref{reformulation}) using the query likelihood model with Dirichlet smoothing~(\( \mu = 1000 \)) in Pyserini.
From this first-stage ranker, we retrieve 100~candidate passages for each query.

\subsection{Argument Quality and Stance Tagging}

After retrieving candidate passages, we tag the argumentative quality and argument stance.
Both quality and stance are required for later steps in our retrieval pipeline to re-rank the passages~(c.f. Section~\ref{reranking}).
Also, the task organizers ask the participants to optionally return a stance for each retrieved document as a sub-task in the Touché Lab on Argument Retrieval.
In order to tag each passage's quality and stance we first split each retrieved passage into sentences using the NLTK library~\cite{BirdLK2009}.
Then each sentence is treated as one potential argument and tagged with its argumentative quality and stance.
To find the quality score and stance for the whole passage, we average the quality or stance scores respectively of all sentences in the passage.

\paragraph{Argument Quality Tagging}
\input{table-quality-mapping.tex}

We implement two different methods for quality tagging:
Our first method is based on the IBM Debater API~\footnote{\url{https://early-access-program.debater.res.ibm.com}}~\cite{ToledoGCFVLJAS2019}.
Here we send each sentence from one passage and the original query as the topic to the IBM Debater API for argument quality assessment. of Passages
The API then determines how good the quality of each argument with regards to the topic is with a \Bert-based regression classifier model trained on the IBM-ArgQ-6.3kArgs dataset. The model and therefore the API then returns a quality score ranging from 0 to 1 where a classified score of~0 means very poor argument quality and a score of~1 means very good argument quality~\cite{ToledoGCFVLJAS2019}.

As a second method to obtain the argument quality we again use the T0++ language model~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021}.
We ask the T0++ model to generate a text the following task: \query{<sentence>.~How would you rate the readability and consistency in this sentence? very good, good, bad, very bad} where \query{<sentence>}~is one sentence of a passage.
This results in an output of either \query{very good}, \query{good}, \query{bad}, or \query{very bad} depending on how the pretrained T0++ model interprets the sentence's argument quality.
We then map this textual output labels to numeric values as per the mapping shown in Table~\ref{table-quality-mapping}.

\paragraph{Argument Stance Tagging}
\input{table-stance-mapping.tex}

Stance detection for each sentence uses the same conceptual approaches but with different inputs and outputs.
Since both the IBM Debater API~\cite{BarHaimBDSS2017} and our T0++ approach~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021} are only capable of calculating a single-target stance~(i.e., for one of the two comparative objects), we combine the two single-target stances into a multi-target stance by taking the difference between the stance towards the first comparative object and the stance towards the second comparative object.
We also experimented with different thresholds for the minimal difference between the single-target stances but found no improvements in the combined, multi-target stance when manually examining some classified examples.

For scoring the single-target argument stance for a sentence with the IBM Debater API, we again send the sentence and a claim built using one of the comparative objects to the IBM Debater API.
The classifier by \citet{BarHaimBDSS2017} computes an argument's likelihood of being pro, con, or neutral with respect to the claim~(i.e., the comparative object in our pipeline) by first classifying sentiments and then detecting contrasts in the topic and argument claim targets.
The API then returns a score from from~-1 to~+1 where -1~means the argument is against the comparative object and +1~means that the argument is in favor of the comparative object.
By classifying different claims for each object~(i.e., \query{<object>~is good} and \query{<object>~is the best}), we get an averaged single-target stance for each comparative object.

For the second method using the T0++~language model~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021} we first experiment with directly asking the model to generate~\query{pro}, \query{con}~or~\query{neutral} classification labels for a comparative object.
However, we found that the T0++ model would not be able to reliably distinguish between a positive and negative stance towards the comparative object.
Therefore we instead reformulate the task in two simple questions, one to determine whether the sentence has a positive stance towards the comparative object and one to determine whether it has a negative stance: \query{<sentence> Is this sentence pro <object>? yes or no} and \query{<sentence> Is this sentence against <object>? yes or no} where \query{<sentence>}~is one sentence of the passage and \query{<object>}~is one of the comparative objects.
This results in two answers~(\query{yes} or~\query{no}) for the positive and negative stance respectively. We combine the two textual answers as shown in Table~\ref{table-stance-mapping} and then combine the single-target stances into a multi-target stance as described above.

\subsection{Axiomatic Reranker}
\label{reranking}

\input{table-axioms.tex}

At the end of our pipeline is the axiomatic reranker which will rerank the retrieved passages from our retrieval step (c.f. Section~\ref{retrieval}).
We use a reranker because in our first pipeline step we increased the recall by using different queries instead of just the original one.
But a good retrieval result set will be precision focused.
The reranker will help us to rearrange the passage in a manner so that the best passages will be at the top of the resulting ranking and therefore we will optimize for measurements that lay their focus on the top results rather than the whole set.
To accomplish this task we will compute preferences between passages via axioms.
Axioms are rules which will vote for or against the original ranking.
By computing axioms between pairs of passages, we can observe which passage should be above or below another passage.
For re-ranking, we use the \iraxioms framework\footnote{\url{https://github.com/webis-de/ir_axioms/}} with the \KwikSort algorithm~\cite{HagenVGS2016}.
In Table \ref{table-axioms} we list the axioms used in our pipeline.

Additional to this reranker which only relies on axioms we also implemented a fairness reranker whose purpose is to produce fair rankings. Fair here means that pro and con arguments appear balanced while preferring subjective arguments over neutral arguments. For this reranking approach, we implemented two different strategies. The first strategy is called alternating stance. Here we have three lists one with only pro arguments regarding comparative object 1, one with only pro arguments regarding comparative object 2, and one with neutral arguments. We then alternately select from the first two lists. If one or both lists are empty we fall back to the neutral list. The second strategy is called balanced top-k stance. Here we count the passages pro the first comparative object and pro the second comparative object in our top-k ranking. If the difference between the two numbers is greater than 1 we move the last pro first comparative object passage from the top-k ranking after the first pro-second comparative object passage after the top-\(k\) ranking.

\subsection{Submitted Runs}

\todo{Shortly describe the 5 submitted runs.}