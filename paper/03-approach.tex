\section{Approach}\label{approach}

\input{figure-pipeline.tex}

We design the architecture of our argumentative retrieval system as a pipeline of multiple steps that subsequently (re-)rank, annotate, or modify the documents or queries given as inputs. This pipeline is shown in Figure~\ref{figure-pipeline}.
We identify four core steps as most important to our approach:
\Ni query expansion, reformulation, and combination,
\Nii first-stage retrieval,
\Niii argument quality and stance tagging,
and \Niv axiomatic reranking and fairness reranking.

Additionally, we add an evaluation component that is not shown in Figure~\ref{figure-pipeline} because it is not needed to retrieve results from our system.
With this evaluation component we can evaluate our system on topics of previous editions~(i.e.,~2021 and~2020) of the Touch√© Lab on Argument Retrieval~(c.f.~Section~\ref{transfer-relevance-judgements}).

\subsection{Query Expansion, Reformulation, and Combination}
\label{reformulation}

In order to increase recall of our first-stage ranker and to include results for very similar yet differently named objects, we first expand and reformulate the original search query.
Our approaches use two different strategies: \Ni we replace the comparative objects with their synonyms and \Nii we generate additional, new queries using the additional description and narrative information provided by the shared task organizers.
Expanding the original query with synonyms of comparative objects is motivated by the fact that documents often contain more specific comparisons~(e.g., Ubuntu vs Windows) instead of more broad comparisons~(e.g., Linux vs Windows).
Yet, specific examples of a more general class of objects are useful to answer comparative questions about their object class.
We might therefore find relevant documents that would otherwise not match any original query term.
It is however important to note that increasing recall can result in an decrease in precision which is undesirable in the precision-oriented setting of the shared task.
However, we later apply re-ranking steps that improve precision by moving irrelevent documents further down the ranking~(c.f.~Section~\ref{reranking}).

We use two different strategies to find synonyms: \Ni word embeddings and \Nii a zero-shot language model.
In our first strategy, we use fastText word embeddings~\cite{BojanowskiGJM2017} to find words with the highest cosine similarity to the given comparative objects in the embedding space.
We manually examine synonyms from fastText embeddings using different domains~(i.e., Wikipedia and Twitter) and find that fastText embeddings trained on the Twitter corpus result in the best synonyms.
Our second strategy to obtain synonyms is based on the T0++~zero-shot language model~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021}.
We ask the model to generate an answer to the following task: \query{What are synonyms of the word <token>?} where \query{<token>} is one of the two comparative objects.
From the text returned by the language model, we then parse synonyms by splitting at commas and remove duplicate synonyms.
With the synonyms returned by either strategy, we replace the comparative objects to form new queries. All alternative queries and the original query are then combined.

\input{table-generated-queries.tex}
We complement the queries expanded by replacing synonyms with newly generated queries that incorporate the contextual information provided in description narrative fields from the shared task's topics.
The description contains important details about the actual information need and the narrative clearly defines which passages are relevant for the query.
We use this valuable information about which passages to retrieve by generating new queries with the T0++ language model~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021} and providing it with a topic's description or narrative.
We challenge the T0++ model with the following task: \query{<text>. Extract a natural search query from this description.} where \query{<text>} is either the topic's narrative or description.
The string returned by the language model is then used as is as the new query for the topic and combined with the previous queries.
In Table~\ref{table-generated-queries}, we show examples of generated queries.
Albeit some of the generated queries~(e.g., topic~53) are just reformulations of the original query, the T0++ language model gernerates some interesting new queries for other topics~(e.g., topic~12).

The query combiner takes the original query, every new computed query from the query reformulation step, and the new queries from the narrative and description fields as inputs and builds an OR-query.
So we will have one query after the first step which contains multiple queries combined with the logical OR operator.
This has some benefits.
Firstly, it is way easier to work with just one query.
This is because multiple queries will result in multiple result sets which have to be interleaved.
Interleaving is not trivial and there are multiple interleaving strategies each with its caveats.
Second, the OR-query will allow us to increase the recall and synonyms which might be not present in the corpus will not result in an empty result set.
In Figure~\ref{figure-pipeline} you can see that there are dotted lines for the query reformulation and combining step.
This means that these steps are optional.
It is possible to use multiple reformulated and expanded new queries from these steps but it is also possible to just use the original query.

\subsection{Retrieval of Passages}\label{retrieval}

To retrieve passages from the set of passages extracted from ClueWeb~12 by the task organizers, we first build an inverted index using the Pyserini framework~\cite{LinMLYPN2021}.
Pyserini allows for experimenting with multiple steps of a retrieval system including indexing and simple retrieval models like Okapi~BM25 or the query likelihood model.

Our index contained the position of index terms, passage vectors, raw passages, stemmed index terms~(Porter stemmer has been used), and stop words had been filtered out by the default Pyserini stopword list.
We then retrieved passages by using the OR-query generated by our query combiner~(c.f. Section~\ref{reformulation}) and using the query likelihood model with Dirichlet smoothing provided by Pyserini with \( \mu = 1000 \).
From this first-stage ranker, we retrieve 100 candidate passages for each query.

\subsection{Argument Quality and Stance Tagging}

After we retrieved the passages we wanted to tag the argument quality and the argument stance.
The quality and stance will be used to rerank the document in the last step of our pipeline.
Detecting the stance of the passages was also one task to accomplish as mentioned in Section~\ref{intro}.
Before we can tag the quality and stance we first have to extract the arguments from the passages.
To do this we used the Python library NLTK~\cite{BirdLK2009} to tokenize the passages into sentences and treated every sentence as one argument.

There are two methods for quality tagging which we used.
The first method is based on the IBM Debater API~\footnote{\url{https://early-access-program.debater.res.ibm.com/academic_use}}.
Here we send all sentences from one passage as arguments and the original query as the topic to the API.
The API then determines how good the quality of each argument with regards to the topic is.
We will get back a quality score ranging from 0 to 1 from the API where 0 means very poor argument quality and 1 means very good argument quality.
\citet{ToledoGCFVLJAS2019} describe which dataset and which methods have been used to develop the argument quality classifier.
A second method to obtain the argument quality is to use T0.
We can ask T0 the following question: \query{<sentence>. How would you rate the readability and consistency in this sentence? very good, good, bad, very bad} where \query{<sentence>} is one sentence of a passage.
This results in \query{very good}, \query{good}, \query{bad}, or \query{very bad} depending on the decision of T0 regarding the argument quality.
We then map the output of T0 to numeric values to ensure a consistent quality calculation.
Very bad will get the value 0, bad the value 0.25, good the value 0.75, and very good the value 1. \todo{Move to table.}

Stance detecting will use the same two different approaches but with different details.
The method via the IBM Debater API works similarly to the version discussed earlier.
We will send each sentence of one passage as arguments and one comparative object as the topic to the API.
The difference now is, that the numeric value returned by the API will range from -1 to 1 where -1 means the argument is against the comparative object and 1 the argument is for the comparative object.
We will do this for both comparative objects to calculate the multi-target stance since the IBM API is only capable of calculating a single-target stance.
After that, we will calculate the mean value of the argument stance returned for one comparative object and one passage.
Then we will build the difference between comparative object 1 and comparative object 2 to determine if the whole passage is for or against one of the comparative objects.
There are some different versions of this stance detecting.
We cannot only use the comparative objects but also sentiments like comparative object 1 is better or comparative object 1 is worse.
We can then build the mean of several stance classifications instead of just one for each comparative object.
It is also possible to work with thresholds to determine when the difference is too low to be counted for or against then we will register the stance as neutral.
\citet{BarHaimBDSS2017} describe which dataset and which methods have been used to develop the argument quality classifier.
For the second method via T0 we ask: \query{<sentence> Is this sentence pro <comparative\_object>? yes or no} and \query{<sentence> Is this sentence against <comparative\_object>? yes or no} where \query{<sentence>} is one sentence of the passage.
This results in two answers \query{yes} or \query{no} which we will map as follows: \todo{Describe T0 stance values.}
We can then compute the stance analogous to the version via the IBM Debater API.

\subsection{Axiomatic Reranker}
\label{reranking}

\input{table-axioms.tex}

At the end of our pipeline is the axiomatic reranker which will rerank the retrieved passages from our retrieval step (c.f. Section~\ref{retrieval}).
We use a reranker because in our first pipeline step we increased the recall by using different queries instead of just the original one.
But a good retrieval result set will be precision focused.
The reranker will help us to rearrange the passage in a manner so that the best passages will be at the top of the resulting ranking and therefore we will optimize for measurements that lay their focus on the top results rather than the whole set.
To accomplish this task we will compute preferences between passages via axioms.
Axioms are rules which will vote for or against the original ranking.
By computing axioms between pairs of passages, we can observe which passage should be above or below another passage.
For re-ranking, we use the \iraxioms framework\footnote{\url{https://github.com/webis-de/ir_axioms/}} with the \KwikSort algorithm~\cite{HagenVGS2016}.
In Table \ref{table-axioms} we list the axioms used in our pipeline.

Additional to this reranker which only relies on axioms we also implemented a fairness reranker whose purpose is to produce fair rankings. Fair here means that pro and con arguments appear balanced while preferring subjective arguments over neutral arguments. For this reranking approach, we implemented two different strategies. The first strategy is called alternating stance. Here we have three lists one with only pro arguments regarding comparative object 1, one with only pro arguments regarding comparative object 2, and one with neutral arguments. We then alternately select from the first two lists. If one or both lists are empty we fall back to the neutral list. The second strategy is called balanced top-k stance. Here we count the passages pro the first comparative object and pro the second comparative object in our top-k ranking. If the difference between the two numbers is greater than 1 we move the last pro first comparative object passage from the top-k ranking after the first pro-second comparative object passage after the top-\(k\) ranking.

\subsection{Submitted Runs}

\todo{Shortly describe the 5 submitted runs.}