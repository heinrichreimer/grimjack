\section{Approach}\label{approach}

In this chapter, we will have a closer look at the different components of our approach.
In Figure~\ref{figure-pipeline} you can see an overview of our approach.
As mentioned in Section~\ref{intro}, our approach consists of four components.
In Figure~\ref{figure-pipeline} there are five components, this is because we implemented an evaluator component to evaluate our results during development.
For evaluation, we used the topics and the corpus from previous years of Touch√©~(2021 and 2020).

\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{figures/pipeline}
\caption{Architecture Overview. Dotted lines mean optional steps.}
\label{figure-pipeline}
\end{figure}

\subsection{Query Reformulation and Query Combiner}\label{reformulation}

In this component, we (a) replaced the comparative objects with synonyms and (b) used the additional information narrative and description provided by the organizers of the shared task to generate additional queries.
Using synonyms will allow us to increase the recall of our initial retrieval step by retrieving passages that contain terms similar to the terms from the original query but would not be retrieved when using only the original query.
With a higher recall, we will be able to find more relevant passages which we will rerank in our last step to improve the precision of our approach.
To find synonyms we used two different methods.
The first method uses word embeddings and the second one uses the language model T0~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021}.

Our first method uses the GloVe~\cite{PenningtonSM2014} embeddings to determine which words have the highest similarity score given the comparative objects.
We use different datasets for the GloVe embeddings which originate from different domains to evaluate which datasets computes the best synonyms for our approach.
We experimented with datasets from Wikipedia and Twitter with different sizes.
The second method to obtain synonyms is based on the language model T0~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021}.
We ask the model the following question: \texttt{What are synonyms of the word <token> ?} where \texttt{<token>} is the comparative object\todo{s}.
The language model will return an answer string with one or more synonyms for the specified token.
Both methods return synonyms for the comparative objects and the new queries will be the original query where the comparative objects have been replaced by these synonyms.
So we will have the original query and the replaced ones.

Another method to generate additional queries is to incorporate the information provided by the narrative and description fields from the topics for the shared task.
Here we can find information about the actual information need and which passages are relevant for the query. So we use this information to generate new queries because this is valuable information about which passages to retrieve.
To generate new queries we again use the language model T0~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021}.
We ask the model the following question: \texttt{<text>. Extract a natural search query from this description.} where \texttt{<text>} is either the topic's narrative or description.
This results in an answer string of the language model which contains a query extracted from the provided text. \todo{Examples for generated queries.}

The query combiner takes the original query, every new computed query from the query reformulation step, and the new queries from the narrative and description fields as inputs and builds an OR-query.
So we will have one query after the first step which contains multiple queries combined with the logical OR operator.
This has some benefits.
Firstly, it is way easier to work with just one query.
This is because multiple queries will result in multiple result sets which have to be interleaved.
Interleaving is not trivial and there are multiple interleaving strategies each with its caveats.
Second, the OR-query will allow us to increase the recall and synonyms which might be not present in the corpus will not result in an empty result set.
In Figure~\ref{fig:pipeline} you can see that there are dotted lines for the query reformulation and combining step.
This means that these steps are optional.
It is possible to use multiple reformulated and expanded new queries from these steps but it is also possible to just use the original query.

\subsection{Retrieval of Passages}\label{retrieval}

To retrieve passages we need to build an index first.
To do this we used the Python library Pyserini~\cite{LinMLYPN2021} which provides an information retrieval tool kit such as indexing and simple retrieval models like Okapi BM25 or the query likelihood model.
Our index contained the position of index terms, passage vectors, raw passages, stemmed index terms~(Porter stemmer has been used), and stop words had been filtered out by the default Pyserini stopword list.
We then retrieved passages by using the OR-query generated by our query combiner~(c.f. Section~\ref{reformulation}) and using the query likelihood model with Dirichlet smoothing provided by Pyserini with \( \mu = 1000 \).
From this first-stage ranker, we retrieve 100 candidate passages for each query.

\subsection{Argument Quality and Stance Tagging}

After we retrieved the passages we wanted to tag the argument quality and the argument stance.
The quality and stance will be used to rerank the document in the last step of our pipeline.
Detecting the stance of the passages was also one task to accomplish as mentioned in Section~\ref{intro}.
Before we can tag the quality and stance we first have to extract the arguments from the passages.
To do this we used the Python library NLTK~\cite{BirdLK2009} to tokenize the passages into sentences and treated every sentence as one argument.

There are two methods for quality tagging which we used.
The first method is based on the IBM Debater API~\footnote{\url{https://early-access-program.debater.res.ibm.com/academic_use}}.
Here we send all sentences from one passage as arguments and the original query as the topic to the API.
The API then determines how good the quality of each argument with regards to the topic is.
We will get back a quality score ranging from 0 to 1 from the API where 0 means very poor argument quality and 1 means very good argument quality.
\citet{ToledoG2019} describe in their paper which dataset and which methods have been used to develop the argument quality classifier.
A second method to obtain the argument quality is to use T0.
We can ask T0 the following question: \texttt{<sentence>. How would you rate the readability and consistency in this sentence? very good, good, bad, very bad} where \texttt{<sentence>} is one sentence of a passage.
This results in \texttt{very good}, \texttt{good}, \texttt{bad}, or \texttt{very bad} depending on the decision of T0 regarding the argument quality.
We then map the output of T0 to numeric values to ensure a consistent quality calculation.
Very bad will get the value 0, bad the value 0.25, good the value 0.75, and very good the value 1. \todo{Move to table.}

Stance detecting will use the same two different approaches but with different details.
The method via the IBM Debater API works similarly to the version discussed earlier.
We will send each sentence of one passage as arguments and one comparative object as the topic to the API.
The difference now is, that the numeric value returned by the API will range from -1 to 1 where -1 means the argument is against the comparative object and 1 the argument is for the comparative object.
We will do this for both comparative objects to calculate the multi-target stance since the IBM API is only capable of calculating a single-target stance.
After that, we will calculate the mean value of the argument stance returned for one comparative object and one passage.
Then we will build the difference between comparative object 1 and comparative object 2 to determine if the whole passage is for or against one of the comparative objects.
There are some different versions of this stance detecting.
We cannot only use the comparative objects but also sentiments like comparative object 1 is better or comparative object 1 is worse.
We can then build the mean of several stance classifications instead of just one for each comparative object.
It is also possible to work with thresholds to determine when the difference is too low to be counted for or against then we will register the stance as neutral.
\citet{BarHaimBDSS2017} describe which dataset and which methods have been used to develop the argument quality classifier.
For the second method via T0 we ask: \texttt{<sentence> Is this sentence pro <comparative\_object>? yes or no} and \texttt{<sentence> Is this sentence against <comparative\_object>? yes or no} where \texttt{<sentence>} is one sentence of the passage.
This results in two answers \texttt{yes} or \texttt{no} which we will map as follows: \todo{Describe T0 stance values.}
We can then compute the stance analogous to the version via the IBM Debater API.

\subsection{Axiomatic Reranker}

At the end of our pipeline is the axiomatic reranker which will rerank the retrieved passages from our retrieval step (c.f. Section~\ref{retrieval}).
We use a reranker because in our first pipeline step we increased the recall by using different queries instead of just the original one.
But a good retrieval result set will be precision focused.
The reranker will help us to rearrange the passage in a manner so that the best passages will be at the top of the resulting ranking and therefore we will optimize for measurements that lay their focus on the top results rather than the whole set.
To accomplish this task we will compute preferences between passages via axioms.
Axioms are rules which will vote for or against the original ranking.
By computing axioms between pairs of passages, we can observe which passage should be above or below another passage.
The actual reranking will be done by the algorithm KwikSort~\cite{hagen:2016d}.
In Table \ref{table-axioms} we list the axioms used in our pipeline.

Additional to this reranker which only relies on axioms we also implemented a fairness reranker whose purpose is to produce fair rankings. Fair here means that pro and con arguments appear balanced while preferring subjective arguments over neutral arguments. For this reranking approach, we implemented two different strategies. The first strategy is called alternating stance. Here we have three lists one with only pro arguments regarding comparative object 1, one with only pro arguments regarding comparative object 2, and one with neutral arguments. We then alternately select from the first two lists. If one or both lists are empty we fall back to the neutral list. The second strategy is called balanced top-k stance. Here we count the passages pro the first comparative object and pro the second comparative object in our top-k ranking. If the difference between the two numbers is greater than 1 we move the last pro first comparative object passage from the top-k ranking after the first pro-second comparative object passage after the top-k ranking.     

\begin{table}
    \caption{Axioms used in our pipeline. Axioms without citations are our own work.}
    \label{table-axioms}
    \begin{tabular}{ll}
        \toprule
        \textbf{Name} & \textbf{Description} \\
        \midrule
        ArgUC~\cite{bondarenko:2018} & Prefer more argumentative units. \\
        QTArg~\cite{bondarenko:2018} & Prefer more query terms in argumentative units. \\
        QTPArg~\cite{bondarenko:2018} & Prefer earlier query terms in argumentative units. \\
        aSL~\cite{bondarenkoaxiomatic} & Prefer passages with 12‚Äì20 words per sentence. \\
        CompArg & Prefer more comparative objects in argumentative units. \\
        CompPArg & Prefer earlier comparative objects in argumentative units. \\
        ArgQ & Prefer higher argument quality. \\ 
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Submitted Runs}

\todo{Shortly describe the 5 submitted runs.}