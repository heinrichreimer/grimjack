\section{Evaluation}
\label{evaluation}

For fast development and evaluation of different parameter settings, we complement our modular retrieval pipeline~(c.f., Section~\refname{approach}) with an evaluation component. Our evaluation module automatically loads relevance judgments from previous editions of the Touché shared tasks on Argument Retrieval~(c.f. Section~\ref{transfer-relevance-judgements}) and transfers the document-level relevance judgments from~2020 and~2021 to the passages from the 2022 dataset.

We submit five runs that use different components and strategies of our approach~(Section~\ref{approach}) to the Touché shared task.
Instead of uploading generated run files, we deploy our retrieval system as a working software via the TIRA platform~\cite{PotthastGWS2019}, as is encouraged by the task organizers~\cite{BondarenkoFKSGBPBSWPH2022}.

\paragraph{Query Likelihood Baseline}

For our first run\footnote{\url{https://tira.io/task/touche-task-2/user/grimjack/dataset/touche-2022-task2/run/2022-05-03-16-21-30}}, we retrieve 100~passages ranked by query likelihood with Dirichlet smoothing~(\(\mu = 1000\)) for the original, unmodified query~\cite{ZhaiL2001}. We then use the IBM fastText TARGER model~\cite{ChernodubOHBHBP2019} to tag argument structure,
the IBM Debater API~\cite{ToledoGCFVLJAS2019} to tag argument quality.
We tag argument stance by comparing sentiments for each object using the IBM Debater API, treating a stance under a threshold of~0.125 as neutral.

\paragraph{Argumentative Axioms}

To generate our second run\footnote{\url{https://tira.io/task/touche-task-2/user/grimjack/dataset/touche-2022-task2/run/2022-05-03-16-56-13}}, we retrieve 100~passages in the same way as for the query likelihood baseline run. Then we re-rank the top-10 passages from the baseline result using \KwikSort~\cite{BondarenkoFRSVH2022,HagenVGS2016} based on preferences from the argumentative axioms as described in Section~\ref{reranking}.

\paragraph{Fair Re-ranking and Argumentative Axioms}

Our third run\footnote{\url{https://tira.io/task/touche-task-2/user/grimjack/dataset/touche-2022-task2/run/2022-05-03-23-52-55}} also uses argumentative axiomatic re-ranking after the baseline retrieval. But to ensure that both comparative objects are fairly represented in the resulting ranking, we apply fairness re-ranking with the alternating stance strategy as described in Section~\ref{reranking}.

\paragraph{All You Need is T0}

With our fourth run\footnote{\url{https://tira.io/task/touche-task-2/user/grimjack/dataset/touche-2022-task2/run/2022-05-05-19-04-26}}, we want to provide a practical example to the recently criticized trend to use more and more language models in search engines~\cite{ShahB2022}.
It is tempting to use a large language model like~T0++~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021}, that can solve many natural language processing tasks with high accuracy, in many steps of a search pipeline. But \citet{ShahB2022} highlight conceptual flaws that question if such an extreme usage of not fully understood models is desirable when answering real-life questions.
We construct a run that uses the language model's zero-shot text generation abilities in as many steps of the pipeline as possible.
First, we generate and combine queries by reformulating new queries from the description and narrative using~T0++ and by replacing synonyms of comparative objects as returned by T0++~(c.f. Section~\ref{reformulation}).
We then retrieve 20~documents by query likelihood, and use T0++ again to tag argument quality and stance~(c.f. Section~\ref{argument-tagging}).

\paragraph{Argumentative Fair Re-ranking with T0}

In our fifth run\footnote{\url{https://tira.io/task/touche-task-2/user/grimjack/dataset/touche-2022-task2/run/2022-05-05-23-53-26}}, we combine most of the approaches introduced in Section~\ref{approach} to generate a ranking that is both as argumentative and fair as possible but also uses T0++~\cite{SanhWRBSACSLRDBXTSSKCNDCJWMSYPBWNRSSFFTBGBWR2021} for query reformulation and expansion.
Here, we also formulate new queries from the description and narrative using~T0++ and expand queries by replacing synonyms returned by T0++. But we also include synonyms by fastText~\cite{BojanowskiGJM2017} embedding similarity~(c.f. Section~\ref{reformulation}).
The top-10 results of the 100 passages retrieved using the query likelihood model for the expanded query are then re-ranked based on argumentative axioms and by alternating stance~(c.f. Section~\ref{reranking}).

% \subsection{T0++ Stance Classification}

% \todo{In camera ready, maybe evaluate T0 stance on the stance dataset provided by the organizers? We could reference this in the description of the T0-run above.}

\subsection{Manual Assessment of Top-5 Results}
% \input{table-judgments}
\input{table-effectiveness}

We assess the effectiveness of our approach by looking at the top-10 retrieved passages in the \emph{Fair Re-ranking and Argumentative Axioms}~(Fair) and \emph{All You Need is T0}~(T0) runs for the three topics~36~(\textquote{What IDE is better for Java: NetBeans or Eclipse?}), 48~(\textquote{Is pasta healthier than pizza?}), and~69~(\textquote{Who was a better boxer, Muhammad Ali or Joe Frazier?}).
We manually judge relevance and stance for the 35~documents pooled from our \emph{Fair Re-ranking and Argumentative Axioms} and \emph{All You Need is T0} runs up to depth~10 for the three topics before the official results are made available by the organizers.
Table~\ref{table-effectiveness} shows the normalized discounted cumulative gain, precision, and mean average precision for the top-10 documents from based on our manual judgments.
Our results indicate that the T0-based approach is slightly better at comparing opinionated queries like topic~69 while the axiomatic and fairness re-ranking approach might improve the effectiveness for more fact-based queries.
We look forward to further investigate this finding when deeper relevance judgments are made available by the organizers.

\subsection{Transferring Relevance Judgements from Touché~2020--2021}
\label{transfer-relevance-judgements}

For a systematic evaluation of all runs with different metrics for effectiveness following the Cranfield paradigm, we also experiment with transferring the relevance judgements from previous editions of the Touché shared task~\cite{BondarenkoFBGAPBSWPH2020,BondarenkoGFBAPBSWPH2021}.
In~2020 and~2021, however, the results were judged on the document level.
Therefore, we using the topics and relevance judgements from Touché~2020--2021, but retrieve passages from the extracted passages in this year's collection~\cite{BondarenkoFKSGBPBSWPH2022}.
Then we assume that if a document was relevant all passages extracted from that document are relevant as well, that is, the passages inherit their containing document's relevance label.
Unfortunately, we could only match 16.76\,\% of the passages retrieved by our baseline with document relevance labels from Touché~2020--2021.
Because of the insufficient label coverage we do not include a more detailed evaluation of effectiveness using previous editions' judgements.
